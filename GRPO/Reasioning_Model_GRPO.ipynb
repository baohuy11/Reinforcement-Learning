{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1afa05927bb24bdb8c162549088878d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_146adb815f6f41d1985a8fdf1e201d88",
              "IPY_MODEL_7a355d97b37d40c39021226b6b0ca838",
              "IPY_MODEL_ba2812d0793240b3806ff08f911ee67e"
            ],
            "layout": "IPY_MODEL_d29ae4e89bc84f658c5e005cddae8e19"
          }
        },
        "146adb815f6f41d1985a8fdf1e201d88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b29ebe553384afeaaa750170685f631",
            "placeholder": "​",
            "style": "IPY_MODEL_0dc6abb1e85e46178300143e394946de",
            "value": "model.safetensors: 100%"
          }
        },
        "7a355d97b37d40c39021226b6b0ca838": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_acffbf724e8a45338a6ff6712784c58a",
            "max": 1503300328,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_720a4f649e2048dea6dc6f092a1ce272",
            "value": 1503300328
          }
        },
        "ba2812d0793240b3806ff08f911ee67e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e76ec86f8164829a367a903e0241e85",
            "placeholder": "​",
            "style": "IPY_MODEL_1fde5ed7cb904cb984e5afe7d8ac5cbf",
            "value": " 1.50G/1.50G [00:05&lt;00:00, 510MB/s]"
          }
        },
        "d29ae4e89bc84f658c5e005cddae8e19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b29ebe553384afeaaa750170685f631": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dc6abb1e85e46178300143e394946de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "acffbf724e8a45338a6ff6712784c58a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "720a4f649e2048dea6dc6f092a1ce272": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7e76ec86f8164829a367a903e0241e85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fde5ed7cb904cb984e5afe7d8ac5cbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b722df9b6c74afdbfb1eef9020ddaa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_afd1d56141c4427c9ff8c8abcc0762d6",
              "IPY_MODEL_610a5b47c8d4431d8c22c3574549f407",
              "IPY_MODEL_9dc2720c43174b34973ca98625b2f568"
            ],
            "layout": "IPY_MODEL_46902ff77489495bbb1b08e957b586d6"
          }
        },
        "afd1d56141c4427c9ff8c8abcc0762d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d9764e762784924ac50f5b1ceca6d9d",
            "placeholder": "​",
            "style": "IPY_MODEL_18f18f2e96384df09652f7f8b43d7b0a",
            "value": "tokenizer.json: 100%"
          }
        },
        "610a5b47c8d4431d8c22c3574549f407": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_285b30ded7be4a6d963a8600b6099ffb",
            "max": 11422654,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e877e949fc9a4891aa5769483c5c6556",
            "value": 11422654
          }
        },
        "9dc2720c43174b34973ca98625b2f568": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_171e43a3eb85484391d8db9ef58d35b7",
            "placeholder": "​",
            "style": "IPY_MODEL_d1c9766517cb406899afe3e0c1de3225",
            "value": " 11.4M/11.4M [00:00&lt;00:00, 69.6kB/s]"
          }
        },
        "46902ff77489495bbb1b08e957b586d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d9764e762784924ac50f5b1ceca6d9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18f18f2e96384df09652f7f8b43d7b0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "285b30ded7be4a6d963a8600b6099ffb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e877e949fc9a4891aa5769483c5c6556": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "171e43a3eb85484391d8db9ef58d35b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1c9766517cb406899afe3e0c1de3225": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4kQkjMBNom7",
        "outputId": "6f4e3c9f-3d60-40c1-e87b-1510af35dd13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "huggingface_hub version: 0.34.4\n",
            "tokenizers version: 0.22.0\n",
            "torch version: 2.8.0+cu126\n"
          ]
        }
      ],
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "pkgs = [\n",
        "    \"huggingface_hub\",\n",
        "    \"tokenizers\",\n",
        "    \"torch\",\n",
        "]\n",
        "\n",
        "for p in pkgs:\n",
        "    print(f\"{p} version: {version(p)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "USE_REASONING_MODEL = True\n",
        "USE_INSTRUCT_MODEL = False"
      ],
      "metadata": {
        "id": "RFUaLHZ1Qqk7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Architecture Code"
      ],
      "metadata": {
        "id": "tVEFjp-9Qchr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "c4rRiZ_IQgWw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
        "        self.fc2 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
        "        self.fc3 = nn.Linear(cfg[\"hidden_dim\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_fc1 = self.fc1(x)\n",
        "        x_fc2 = self.fc2(x)\n",
        "        x = nn.functional.silu(x_fc1) * x_fc2\n",
        "        return self.fc3(x)"
      ],
      "metadata": {
        "id": "npVjgkdlQkOL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, emb_dim, eps=1e-6, bias=False, qwen3_compatible=True):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.qwen3_compatible = qwen3_compatible\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim)) if bias else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        input_dtype = x.dtype\n",
        "\n",
        "        if self.qwen3_compatible:\n",
        "            x = x.to(torch.float32)\n",
        "\n",
        "        variance = x.pow(2).mean(dim=-1, keepdim=True)\n",
        "        norm_x = x * torch.rsqrt(variance + self.eps)\n",
        "        norm_x = norm_x * self.scale\n",
        "\n",
        "        if self.shift is not None:\n",
        "            norm_x = norm_x + self.shift\n",
        "\n",
        "        return norm_x.to(input_dtype)"
      ],
      "metadata": {
        "id": "bv3kJ0nqRTrN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_rope_params(head_dim, theta_base=10_000, context_length=4096, dtype=torch.float32):\n",
        "    assert head_dim % 2 == 0, \"Embedding dimension must be even\"\n",
        "\n",
        "    # Compute the inverse frequencies\n",
        "    inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2, dtype=dtype)[: (head_dim // 2)].float() / head_dim))\n",
        "\n",
        "    # Generate position indices\n",
        "    positions = torch.arange(context_length, dtype=dtype)\n",
        "\n",
        "    # Compute the angles\n",
        "    angles = positions[:, None] * inv_freq[None, :]  # Shape: (context_length, head_dim // 2)\n",
        "\n",
        "    # Expand angles to match the head_dim\n",
        "    angles = torch.cat([angles, angles], dim=1)  # Shape: (context_length, head_dim)\n",
        "\n",
        "    # Precompute sine and cosine\n",
        "    cos = torch.cos(angles)\n",
        "    sin = torch.sin(angles)\n",
        "\n",
        "    return cos, sin\n",
        "\n",
        "\n",
        "def apply_rope(x, cos, sin, offset=0):\n",
        "    # x: (batch_size, num_heads, seq_len, head_dim)\n",
        "    batch_size, num_heads, seq_len, head_dim = x.shape\n",
        "    assert head_dim % 2 == 0, \"Head dimension must be even\"\n",
        "\n",
        "    # Split x into first half and second half\n",
        "    x1 = x[..., : head_dim // 2]  # First half\n",
        "    x2 = x[..., head_dim // 2:]  # Second half\n",
        "\n",
        "    # Adjust sin and cos shapes\n",
        "    cos = cos[offset:offset + seq_len, :].unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, seq_len, head_dim // 2)\n",
        "    sin = sin[offset:offset + seq_len, :].unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    # Apply the rotary transformation\n",
        "    rotated = torch.cat((-x2, x1), dim=-1)\n",
        "    x_rotated = (x * cos) + (rotated * sin)\n",
        "\n",
        "    # It's ok to use lower-precision after applying cos and sin rotation\n",
        "    return x_rotated.to(dtype=x.dtype)"
      ],
      "metadata": {
        "id": "_RJuVL7GRaZe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GroupedQueryAttention(nn.Module):\n",
        "    def __init__(\n",
        "        self, d_in, num_heads, num_kv_groups, head_dim=None, qk_norm=False, dtype=None\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert num_heads % num_kv_groups == 0, \"num_heads must be divisible by num_kv_groups\"\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.num_kv_groups = num_kv_groups\n",
        "        self.group_size = num_heads // num_kv_groups\n",
        "\n",
        "        if head_dim is None:\n",
        "            assert d_in % num_heads == 0, \"`d_in` must be divisible by `num_heads` if `head_dim` is not set\"\n",
        "            head_dim = d_in // num_heads\n",
        "\n",
        "        self.head_dim = head_dim\n",
        "        self.d_out = num_heads * head_dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, self.d_out, bias=False, dtype=dtype)\n",
        "        self.W_key = nn.Linear(d_in, num_kv_groups * head_dim, bias=False, dtype=dtype)\n",
        "        self.W_value = nn.Linear(d_in, num_kv_groups * head_dim, bias=False, dtype=dtype)\n",
        "\n",
        "        self.out_proj = nn.Linear(self.d_out, d_in, bias=False, dtype=dtype)\n",
        "\n",
        "        if qk_norm:\n",
        "            self.q_norm = RMSNorm(head_dim, eps=1e-6)\n",
        "            self.k_norm = RMSNorm(head_dim, eps=1e-6)\n",
        "        else:\n",
        "            self.q_norm = self.k_norm = None\n",
        "\n",
        "    def forward(self, x, mask, cos, sin, start_pos=0, cache=None):\n",
        "        b, num_tokens, _ = x.shape\n",
        "\n",
        "        # Apply projections\n",
        "        queries = self.W_query(x)  # (b, num_tokens, num_heads * head_dim)\n",
        "        keys = self.W_key(x)       # (b, num_tokens, num_kv_groups * head_dim)\n",
        "        values = self.W_value(x)   # (b, num_tokens, num_kv_groups * head_dim)\n",
        "\n",
        "        # Reshape to heads / kv-groups\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        keys_new = keys.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2)\n",
        "        values_new = values.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Optional normalization\n",
        "        if self.q_norm:\n",
        "            queries = self.q_norm(queries)\n",
        "        if self.k_norm:\n",
        "            keys_new = self.k_norm(keys_new)\n",
        "\n",
        "        # Apply RoPE\n",
        "        queries = apply_rope(queries, cos, sin, offset=start_pos)\n",
        "        keys_new = apply_rope(keys_new, cos, sin, offset=start_pos)\n",
        "\n",
        "        if cache is not None:\n",
        "            prev_k, prev_v = cache\n",
        "            keys = torch.cat([prev_k, keys_new], dim=2)\n",
        "            values = torch.cat([prev_v, values_new], dim=2)\n",
        "        else:\n",
        "            start_pos = 0  # reset RoPE\n",
        "            keys, values = keys_new, values_new\n",
        "        next_cache = (keys, values)\n",
        "\n",
        "        # Expand K and V to match number of heads\n",
        "        keys = keys.repeat_interleave(self.group_size, dim=1)\n",
        "        values = values.repeat_interleave(self.group_size, dim=1)\n",
        "\n",
        "        # Attention\n",
        "        attn_scores = queries @ keys.transpose(2, 3)\n",
        "        attn_scores = attn_scores.masked_fill(mask, -torch.inf)\n",
        "        attn_weights = torch.softmax(attn_scores / self.head_dim**0.5, dim=-1)\n",
        "\n",
        "        context = (attn_weights @ values).transpose(1, 2).reshape(b, num_tokens, self.d_out)\n",
        "        return self.out_proj(context), next_cache"
      ],
      "metadata": {
        "id": "eqIgTDPPRf0m"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = GroupedQueryAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            head_dim=cfg[\"head_dim\"],\n",
        "            num_kv_groups=cfg[\"n_kv_groups\"],\n",
        "            qk_norm=cfg[\"qk_norm\"],\n",
        "            dtype=cfg[\"dtype\"]\n",
        "        )\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
        "        self.norm2 = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
        "\n",
        "    def forward(self, x, mask, cos, sin, start_pos=0, cache=None):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x, next_cache = self.att(x, mask, cos, sin, start_pos=start_pos, cache=cache)  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed-forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x, next_cache"
      ],
      "metadata": {
        "id": "_PotGWi6RgX2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Qwen3Model(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "\n",
        "        # Main model parameters\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"])\n",
        "\n",
        "        self.trf_blocks = nn.ModuleList(  # ModuleList since Sequential can only accept one input, and we need `x, mask, cos, sin`\n",
        "            [TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
        "        )\n",
        "        self.final_norm = RMSNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False, dtype=cfg[\"dtype\"])\n",
        "\n",
        "        # Reusable utilities\n",
        "        if cfg[\"head_dim\"] is None:\n",
        "            head_dim = cfg[\"emb_dim\"] // cfg[\"n_heads\"]\n",
        "        else:\n",
        "            head_dim = cfg[\"head_dim\"]\n",
        "        cos, sin = compute_rope_params(\n",
        "            head_dim=head_dim,\n",
        "            theta_base=cfg[\"rope_base\"],\n",
        "            context_length=cfg[\"context_length\"]\n",
        "        )\n",
        "        self.register_buffer(\"cos\", cos, persistent=False)\n",
        "        self.register_buffer(\"sin\", sin, persistent=False)\n",
        "        self.cfg = cfg\n",
        "        self.current_pos = 0  # Track current position in KV cache\n",
        "\n",
        "    def forward(self, in_idx, cache=None):\n",
        "        # Forward pass\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        x = tok_embeds\n",
        "\n",
        "        num_tokens = x.shape[1]\n",
        "        if cache is not None:\n",
        "            pos_start = self.current_pos\n",
        "            pos_end = pos_start + num_tokens\n",
        "            self.current_pos = pos_end\n",
        "            mask = torch.triu(\n",
        "                torch.ones(pos_end, pos_end, device=x.device, dtype=torch.bool), diagonal=1\n",
        "            )[pos_start:pos_end, :pos_end]\n",
        "        else:\n",
        "            pos_start = 0  # Not strictly necessary but helps torch.compile\n",
        "            mask = torch.triu(\n",
        "                torch.ones(num_tokens, num_tokens, device=x.device, dtype=torch.bool), diagonal=1\n",
        "            )\n",
        "        # Shape (1, 1, num_tokens, num_tokens) to broadcast across batch and heads\n",
        "        mask = mask[None, None, :, :]  # broadcast mask\n",
        "\n",
        "        next_cache = []\n",
        "        for i, block in enumerate(self.trf_blocks):\n",
        "            blk_cache = cache.get(i) if cache else None\n",
        "            x, new_blk_cache = block(x, mask, self.cos, self.sin,\n",
        "                                     start_pos=pos_start,\n",
        "                                     cache=blk_cache)\n",
        "            if cache is not None:\n",
        "                cache.update(i, new_blk_cache)\n",
        "            next_cache.append(new_blk_cache)\n",
        "\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x.to(self.cfg[\"dtype\"]))\n",
        "        return logits\n",
        "\n",
        "    def reset_kv_cache(self):\n",
        "        self.current_pos = 0"
      ],
      "metadata": {
        "id": "1dICall1R5gE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KVCache:\n",
        "    def __init__(self, n_layers):\n",
        "        self.cache = [None] * n_layers\n",
        "\n",
        "    def get(self, layer_idx):\n",
        "        return self.cache[layer_idx]\n",
        "\n",
        "    def update(self, layer_idx, value):\n",
        "        self.cache[layer_idx] = value\n",
        "\n",
        "    def get_all(self):\n",
        "        return self.cache\n",
        "\n",
        "    def reset(self):\n",
        "        for i in range(len(self.cache)):\n",
        "            self.cache[i] = None"
      ],
      "metadata": {
        "id": "26GfWOWvR9ca"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Initialize model"
      ],
      "metadata": {
        "id": "ERCZgt_cSBPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CHOOSE_MODEL = \"0.6B\"\n",
        "\n",
        "if CHOOSE_MODEL == \"0.6B\":\n",
        "    QWEN3_CONFIG = {\n",
        "        \"vocab_size\": 151_936,           # Vocabulary size\n",
        "        \"context_length\": 40_960,        # Context length that was used to train the model\n",
        "        \"emb_dim\": 1024,                 # Embedding dimension\n",
        "        \"n_heads\": 16,                   # Number of attention heads\n",
        "        \"n_layers\": 28,                  # Number of layers\n",
        "        \"hidden_dim\": 3072,              # Size of the intermediate dimension in FeedForward\n",
        "        \"head_dim\": 128,                 # Size of the heads in GQA\n",
        "        \"qk_norm\": True,                 # Whether to normalize queries and keys in GQA\n",
        "        \"n_kv_groups\": 8,                # Key-Value groups for grouped-query attention\n",
        "        \"rope_base\": 1_000_000.0,        # The base in RoPE's \"theta\"\n",
        "        \"dtype\": torch.bfloat16,         # Lower-precision dtype to reduce memory usage\n",
        "    }\n",
        "\n",
        "elif CHOOSE_MODEL == \"1.7B\":\n",
        "    QWEN3_CONFIG = {\n",
        "        \"vocab_size\": 151_936,\n",
        "        \"context_length\": 40_960,\n",
        "        \"emb_dim\": 2048,                 # 2x larger than above\n",
        "        \"n_heads\": 16,\n",
        "        \"n_layers\": 28,\n",
        "        \"hidden_dim\": 6144,              # 2x larger than above\n",
        "        \"head_dim\": 128,\n",
        "        \"qk_norm\": True,\n",
        "        \"n_kv_groups\": 8,\n",
        "        \"rope_base\": 1_000_000.0,\n",
        "        \"dtype\": torch.bfloat16,\n",
        "    }\n",
        "\n",
        "elif CHOOSE_MODEL == \"4B\":\n",
        "    QWEN3_CONFIG = {\n",
        "        \"vocab_size\": 151_936,\n",
        "        \"context_length\": 40_960,\n",
        "        \"emb_dim\": 2560,                 # 25% larger than above\n",
        "        \"n_heads\": 32,                   # 2x larger than above\n",
        "        \"n_layers\": 36,                  # 29% larger than above\n",
        "        \"hidden_dim\": 9728,              # ~3x larger than above\n",
        "        \"head_dim\": 128,\n",
        "        \"qk_norm\": True,\n",
        "        \"n_kv_groups\": 8,\n",
        "        \"rope_base\": 1_000_000.0,\n",
        "        \"dtype\": torch.bfloat16,\n",
        "    }\n",
        "\n",
        "elif CHOOSE_MODEL == \"8B\":\n",
        "    QWEN3_CONFIG = {\n",
        "        \"vocab_size\": 151_936,\n",
        "        \"context_length\": 40_960,\n",
        "        \"emb_dim\": 4096,                 # 60% larger than above\n",
        "        \"n_heads\": 32,\n",
        "        \"n_layers\": 36,                  # 26% larger than above\n",
        "        \"hidden_dim\": 12288,\n",
        "        \"head_dim\": 128,\n",
        "        \"qk_norm\": True,\n",
        "        \"n_kv_groups\": 8,\n",
        "        \"rope_base\": 1_000_000.0,\n",
        "        \"dtype\": torch.bfloat16,\n",
        "    }\n",
        "\n",
        "elif CHOOSE_MODEL == \"14B\":\n",
        "    QWEN3_CONFIG = {\n",
        "        \"vocab_size\": 151_936,\n",
        "        \"context_length\": 40_960,\n",
        "        \"emb_dim\": 5120,                 # 25% larger than above\n",
        "        \"n_heads\": 40,                   # 25% larger than above\n",
        "        \"n_layers\": 40,                  # 11% larger than above\n",
        "        \"hidden_dim\": 17408,             # 42% larger than above\n",
        "        \"head_dim\": 128,\n",
        "        \"qk_norm\": True,\n",
        "        \"n_kv_groups\": 8,\n",
        "        \"rope_base\": 1_000_000.0,\n",
        "        \"dtype\": torch.bfloat16,\n",
        "    }\n",
        "\n",
        "elif CHOOSE_MODEL == \"32B\":\n",
        "    QWEN3_CONFIG = {\n",
        "        \"vocab_size\": 151_936,\n",
        "        \"context_length\": 40_960,\n",
        "        \"emb_dim\": 5120,\n",
        "        \"n_heads\": 64,                   # 60% larger than above\n",
        "        \"n_layers\": 64,                  # 60% larger than above\n",
        "        \"hidden_dim\": 25600,             # 47% larger than above\n",
        "        \"head_dim\": 128,\n",
        "        \"qk_norm\": True,\n",
        "        \"n_kv_groups\": 8,\n",
        "        \"rope_base\": 1_000_000.0,\n",
        "        \"dtype\": torch.bfloat16,\n",
        "    }\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"{CHOOSE_MODEL} is not supported.\")"
      ],
      "metadata": {
        "id": "9PpPmLRNhYQK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "model = Qwen3Model(QWEN3_CONFIG)"
      ],
      "metadata": {
        "id": "N_aV5IXEhcAB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OI4pX6FiheeW",
        "outputId": "e3aaf4a3-8428-401d-a0c9-a00c108e9b69"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Qwen3Model(\n",
              "  (tok_emb): Embedding(151936, 1024)\n",
              "  (trf_blocks): ModuleList(\n",
              "    (0-27): 28 x TransformerBlock(\n",
              "      (att): GroupedQueryAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=2048, bias=False)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "        (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
              "        (q_norm): RMSNorm()\n",
              "        (k_norm): RMSNorm()\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (fc1): Linear(in_features=1024, out_features=3072, bias=False)\n",
              "        (fc2): Linear(in_features=1024, out_features=3072, bias=False)\n",
              "        (fc3): Linear(in_features=3072, out_features=1024, bias=False)\n",
              "      )\n",
              "      (norm1): RMSNorm()\n",
              "      (norm2): RMSNorm()\n",
              "    )\n",
              "  )\n",
              "  (final_norm): RMSNorm()\n",
              "  (out_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params:,}\")\n",
        "\n",
        "# Account for weight tying\n",
        "total_params_normalized = total_params - model.tok_emb.weight.numel()\n",
        "print(f\"\\nTotal number of unique parameters: {total_params_normalized:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xt7WyIIWhrN-",
        "outputId": "1ca6425f-414a-4fe9-cfb0-51ecb7713999"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 751,632,384\n",
            "\n",
            "Total number of unique parameters: 596,049,920\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def model_memory_size(model, input_dtype=torch.float32):\n",
        "    total_params = 0\n",
        "    total_grads = 0\n",
        "    for param in model.parameters():\n",
        "        # Calculate total number of elements per parameter\n",
        "        param_size = param.numel()\n",
        "        total_params += param_size\n",
        "        # Check if gradients are stored for this parameter\n",
        "        if param.requires_grad:\n",
        "            total_grads += param_size\n",
        "\n",
        "    # Calculate buffer size (non-parameters that require memory)\n",
        "    total_buffers = sum(buf.numel() for buf in model.buffers())\n",
        "\n",
        "    # Size in bytes = (Number of elements) * (Size of each element in bytes)\n",
        "    # We assume parameters and gradients are stored in the same type as input dtype\n",
        "    element_size = torch.tensor(0, dtype=input_dtype).element_size()\n",
        "    total_memory_bytes = (total_params + total_grads + total_buffers) * element_size\n",
        "\n",
        "    # Convert bytes to gigabytes\n",
        "    total_memory_gb = total_memory_bytes / (1024**3)\n",
        "\n",
        "    return total_memory_gb"
      ],
      "metadata": {
        "id": "zRLbt7mvht2u"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "model.to(device);"
      ],
      "metadata": {
        "id": "Of4IsLSLhwqa"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Load pretrained weights"
      ],
      "metadata": {
        "id": "IL1Uy0oJhxAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_weights_into_qwen(model, param_config, params):\n",
        "    def assign(left, right, tensor_name=\"unknown\"):\n",
        "        if left.shape != right.shape:\n",
        "            raise ValueError(f\"Shape mismatch in tensor '{tensor_name}'. Left: {left.shape}, Right: {right.shape}\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if isinstance(right, torch.Tensor):\n",
        "                left.copy_(right)\n",
        "            else:\n",
        "                left.copy_(torch.as_tensor(right, dtype=left.dtype, device=left.device))\n",
        "\n",
        "        return left\n",
        "\n",
        "    model.tok_emb.weight = assign(model.tok_emb.weight, params[\"model.embed_tokens.weight\"], \"model.embed_tokens.weight\")\n",
        "\n",
        "    for l in range(param_config[\"n_layers\"]):\n",
        "        block = model.trf_blocks[l]\n",
        "        att = block.att\n",
        "\n",
        "        # Q, K, V projections\n",
        "        att.W_query.weight = assign(\n",
        "            att.W_query.weight,\n",
        "            params[f\"model.layers.{l}.self_attn.q_proj.weight\"],\n",
        "            f\"model.layers.{l}.self_attn.q_proj.weight\"\n",
        "        )\n",
        "        att.W_key.weight = assign(\n",
        "            att.W_key.weight,\n",
        "            params[f\"model.layers.{l}.self_attn.k_proj.weight\"],\n",
        "            f\"model.layers.{l}.self_attn.k_proj.weight\"\n",
        "        )\n",
        "        att.W_value.weight = assign(\n",
        "            att.W_value.weight,\n",
        "            params[f\"model.layers.{l}.self_attn.v_proj.weight\"],\n",
        "            f\"model.layers.{l}.self_attn.v_proj.weight\"\n",
        "        )\n",
        "\n",
        "        # Output projection\n",
        "        att.out_proj.weight = assign(\n",
        "            att.out_proj.weight,\n",
        "            params[f\"model.layers.{l}.self_attn.o_proj.weight\"],\n",
        "            f\"model.layers.{l}.self_attn.o_proj.weight\"\n",
        "        )\n",
        "\n",
        "        # QK norms\n",
        "        if hasattr(att, \"q_norm\") and att.q_norm is not None:\n",
        "            att.q_norm.scale = assign(\n",
        "                att.q_norm.scale,\n",
        "                params[f\"model.layers.{l}.self_attn.q_norm.weight\"],\n",
        "                f\"model.layers.{l}.self_attn.q_norm.weight\"\n",
        "            )\n",
        "        if hasattr(att, \"k_norm\") and att.k_norm is not None:\n",
        "            att.k_norm.scale = assign(\n",
        "                att.k_norm.scale,\n",
        "                params[f\"model.layers.{l}.self_attn.k_norm.weight\"],\n",
        "                f\"model.layers.{l}.self_attn.k_norm.weight\"\n",
        "            )\n",
        "\n",
        "        # Attention layernorm\n",
        "        block.norm1.scale = assign(\n",
        "            block.norm1.scale,\n",
        "            params[f\"model.layers.{l}.input_layernorm.weight\"],\n",
        "            f\"model.layers.{l}.input_layernorm.weight\"\n",
        "        )\n",
        "\n",
        "        # Feedforward weights\n",
        "        block.ff.fc1.weight = assign(\n",
        "            block.ff.fc1.weight,\n",
        "            params[f\"model.layers.{l}.mlp.gate_proj.weight\"],\n",
        "            f\"model.layers.{l}.mlp.gate_proj.weight\"\n",
        "        )\n",
        "        block.ff.fc2.weight = assign(\n",
        "            block.ff.fc2.weight,\n",
        "            params[f\"model.layers.{l}.mlp.up_proj.weight\"],\n",
        "            f\"model.layers.{l}.mlp.up_proj.weight\"\n",
        "        )\n",
        "        block.ff.fc3.weight = assign(\n",
        "            block.ff.fc3.weight,\n",
        "            params[f\"model.layers.{l}.mlp.down_proj.weight\"],\n",
        "            f\"model.layers.{l}.mlp.down_proj.weight\"\n",
        "        )\n",
        "        block.norm2.scale = assign(\n",
        "            block.norm2.scale,\n",
        "            params[f\"model.layers.{l}.post_attention_layernorm.weight\"],\n",
        "            f\"model.layers.{l}.post_attention_layernorm.weight\"\n",
        "        )\n",
        "\n",
        "    # Final normalization and output head\n",
        "    model.final_norm.scale = assign(model.final_norm.scale, params[\"model.norm.weight\"], \"model.norm.weight\")\n",
        "\n",
        "    if \"lm_head.weight\" in params:\n",
        "        model.out_head.weight = assign(model.out_head.weight, params[\"lm_head.weight\"], \"lm_head.weight\")\n",
        "    else:\n",
        "        model.out_head.weight = model.tok_emb.weight\n",
        "        print(\"Model uses weight tying.\")"
      ],
      "metadata": {
        "id": "2c31aMAgh2pv"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from safetensors.torch import load_file\n",
        "from huggingface_hub import hf_hub_download, snapshot_download\n",
        "\n",
        "\n",
        "if USE_REASONING_MODEL:\n",
        "    repo_id = f\"Qwen/Qwen3-{CHOOSE_MODEL}\"\n",
        "else:\n",
        "    repo_id = f\"Qwen/Qwen3-{CHOOSE_MODEL}-Base\"\n",
        "\n",
        "local_dir = Path(repo_id).parts[-1]\n",
        "\n",
        "if CHOOSE_MODEL == \"0.6B\":\n",
        "    weights_file = hf_hub_download(\n",
        "        repo_id=repo_id,\n",
        "        filename=\"model.safetensors\",\n",
        "        local_dir=local_dir,\n",
        "    )\n",
        "    weights_dict = load_file(weights_file)\n",
        "else:\n",
        "    repo_dir = snapshot_download(repo_id=repo_id, local_dir=local_dir)\n",
        "    index_path = os.path.join(repo_dir, \"model.safetensors.index.json\")\n",
        "    with open(index_path, \"r\") as f:\n",
        "        index = json.load(f)\n",
        "\n",
        "    weights_dict = {}\n",
        "    for filename in set(index[\"weight_map\"].values()):\n",
        "        shard_path = os.path.join(repo_dir, filename)\n",
        "        shard = load_file(shard_path)\n",
        "        weights_dict.update(shard)\n",
        "\n",
        "load_weights_into_qwen(model, QWEN3_CONFIG, weights_dict)\n",
        "model.to(device)\n",
        "del weights_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "1afa05927bb24bdb8c162549088878d1",
            "146adb815f6f41d1985a8fdf1e201d88",
            "7a355d97b37d40c39021226b6b0ca838",
            "ba2812d0793240b3806ff08f911ee67e",
            "d29ae4e89bc84f658c5e005cddae8e19",
            "2b29ebe553384afeaaa750170685f631",
            "0dc6abb1e85e46178300143e394946de",
            "acffbf724e8a45338a6ff6712784c58a",
            "720a4f649e2048dea6dc6f092a1ce272",
            "7e76ec86f8164829a367a903e0241e85",
            "1fde5ed7cb904cb984e5afe7d8ac5cbf"
          ]
        },
        "id": "oQZLlmymh3F3",
        "outputId": "96389636-529c-41f5-d6c4-9a00327ecb6e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.50G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1afa05927bb24bdb8c162549088878d1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Load Tokenizer"
      ],
      "metadata": {
        "id": "qg27fc0hh7ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from tokenizers import Tokenizer\n",
        "\n",
        "class Qwen3Tokenizer:\n",
        "    _SPECIALS = [\n",
        "        \"<|endoftext|>\",\n",
        "        \"<|im_start|>\", \"<|im_end|>\",\n",
        "        \"<|object_ref_start|>\", \"<|object_ref_end|>\",\n",
        "        \"<|box_start|>\", \"<|box_end|>\",\n",
        "        \"<|quad_start|>\", \"<|quad_end|>\",\n",
        "        \"<|vision_start|>\", \"<|vision_end|>\",\n",
        "        \"<|vision_pad|>\", \"<|image_pad|>\", \"<|video_pad|>\",\n",
        "        \"<think>\", \"</think>\"\n",
        "    ]\n",
        "    _SPLIT_RE = re.compile(r\"(<\\|[^>]+?\\|>|<think>|</think>)\")\n",
        "\n",
        "    def __init__(self, tokenizer_file_path=\"tokenizer.json\", repo_id=None,\n",
        "                 apply_chat_template=True, add_generation_prompt=False, add_thinking=False):\n",
        "\n",
        "        self.apply_chat_template = apply_chat_template\n",
        "        self.add_generation_prompt = add_generation_prompt\n",
        "        self.add_thinking = add_thinking\n",
        "\n",
        "        tok_file = Path(tokenizer_file_path)\n",
        "        self._tok = Tokenizer.from_file(str(tok_file))\n",
        "        self._special_to_id = {}\n",
        "        for t in self._SPECIALS:\n",
        "            tid = self._tok.token_to_id(t)\n",
        "            if tid is not None:\n",
        "                self._special_to_id[t] = tid\n",
        "\n",
        "        self.pad_token_id = self._special_to_id[\"<|endoftext|>\"]\n",
        "        self.eos_token_id = self.pad_token_id\n",
        "\n",
        "        if repo_id and \"Base\" not in repo_id:\n",
        "            eos_token = \"<|im_end|>\"\n",
        "        else:\n",
        "            eos_token = \"<|endoftext|>\"\n",
        "        if eos_token in self._special_to_id:\n",
        "            self.eos_token_id = self._special_to_id[eos_token]\n",
        "\n",
        "    def encode(self, text, chat_wrapped=None):\n",
        "        if chat_wrapped is None:\n",
        "            chat_wrapped = self.apply_chat_template\n",
        "\n",
        "        stripped = text.strip()\n",
        "        if stripped in self._special_to_id and \"\\n\" not in stripped:\n",
        "            return [self._special_to_id[stripped]]\n",
        "\n",
        "        if chat_wrapped:\n",
        "            text = self._wrap_chat(text)\n",
        "\n",
        "        ids = []\n",
        "        for part in filter(None, self._SPLIT_RE.split(text)):\n",
        "            if part in self._special_to_id:\n",
        "                ids.append(self._special_to_id[part])\n",
        "            else:\n",
        "                ids.extend(self._tok.encode(part).ids)\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        return self._tok.decode(ids, skip_special_tokens=False)\n",
        "\n",
        "    def _wrap_chat(self, user_msg):\n",
        "        s = f\"<|im_start|>user\\n{user_msg}<|im_end|>\\n\"\n",
        "        if self.add_generation_prompt:\n",
        "            s += \"<|im_start|>assistant\"\n",
        "            if self.add_thinking:\n",
        "                s += \"\\n\"\n",
        "            else:\n",
        "                s += \"\\n<think>\\n\\n</think>\\n\\n\"\n",
        "        return s"
      ],
      "metadata": {
        "id": "E8qQWYwmiBPn"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if USE_REASONING_MODEL:\n",
        "    tokenizer_file_path = f\"Qwen3-{CHOOSE_MODEL}/tokenizer.json\"\n",
        "else:\n",
        "    tokenizer_file_path = f\"Qwen3-{CHOOSE_MODEL}-Base/tokenizer.json\"\n",
        "\n",
        "hf_hub_download(\n",
        "    repo_id=repo_id,\n",
        "    filename=\"tokenizer.json\",\n",
        "    local_dir=local_dir,\n",
        ")\n",
        "\n",
        "tokenizer = Qwen3Tokenizer(\n",
        "    tokenizer_file_path=tokenizer_file_path,\n",
        "    repo_id=repo_id,\n",
        "    apply_chat_template=USE_REASONING_MODEL,\n",
        "    add_generation_prompt=USE_REASONING_MODEL,\n",
        "    add_thinking=not USE_INSTRUCT_MODEL\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "1b722df9b6c74afdbfb1eef9020ddaa3",
            "afd1d56141c4427c9ff8c8abcc0762d6",
            "610a5b47c8d4431d8c22c3574549f407",
            "9dc2720c43174b34973ca98625b2f568",
            "46902ff77489495bbb1b08e957b586d6",
            "4d9764e762784924ac50f5b1ceca6d9d",
            "18f18f2e96384df09652f7f8b43d7b0a",
            "285b30ded7be4a6d963a8600b6099ffb",
            "e877e949fc9a4891aa5769483c5c6556",
            "171e43a3eb85484391d8db9ef58d35b7",
            "d1c9766517cb406899afe3e0c1de3225"
          ]
        },
        "id": "PRc5IBHxiEtw",
        "outputId": "4b19e223-84d8-4fe2-e96c-2dddadfe6bd3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b722df9b6c74afdbfb1eef9020ddaa3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Give me a short introduction to large language models.\"\n",
        "\n",
        "input_token_ids = tokenizer.encode(prompt)\n",
        "text = tokenizer.decode(input_token_ids)\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "H_XqJTcWiFTQ",
        "outputId": "c7083576-fa7c-4eb4-9c76-03ce3297e624"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|im_start|>user\\nGive me a short introduction to large language models.<|im_end|>\\n<|im_start|>assistant\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Generate text"
      ],
      "metadata": {
        "id": "UkepHJIZiHcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_basic_stream(model, token_ids, max_new_tokens, eos_token_id=None, context_size=None):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        cache = KVCache(n_layers=model.cfg[\"n_layers\"])\n",
        "        model.reset_kv_cache()\n",
        "\n",
        "        # Prime the cache with the initial context\n",
        "        logits = model(token_ids, cache=cache)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            next_token = torch.argmax(logits[:, -1], dim=-1, keepdim=True)\n",
        "\n",
        "            if eos_token_id is not None and torch.all(next_token == eos_token_id):\n",
        "                break\n",
        "\n",
        "            yield next_token\n",
        "\n",
        "            token_ids = torch.cat([token_ids, next_token], dim=1)\n",
        "\n",
        "            # Feed only the new token to the model; cache handles history\n",
        "            logits = model(next_token, cache=cache)"
      ],
      "metadata": {
        "id": "e-0CGeb9iLSD"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_token_ids_tensor = torch.tensor(input_token_ids, device=device).unsqueeze(0)\n",
        "\n",
        "for token in generate_text_basic_stream(\n",
        "    model=model,\n",
        "    token_ids=input_token_ids_tensor,\n",
        "    max_new_tokens=500,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        "):\n",
        "    token_id = token.squeeze(0).tolist()\n",
        "    print(\n",
        "        tokenizer.decode(token_id),\n",
        "        end=\"\",\n",
        "        flush=True\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYHW2Y52iNSu",
        "outputId": "fc94a98b-0cec-479b-af66-1455f160f8f6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<think>\n",
            "Okay, the user wants a short introduction to large language models. Let me start by recalling what I know. Large language models are AI systems that can understand and generate human language. They're trained on massive datasets, so they can learn complex patterns and nuances.\n",
            "\n",
            "I should mention their ability to understand and generate text, not just specific tasks. Maybe include examples like chatbots or language assistants. Also, emphasize their adaptability and efficiency. Oh, and maybe touch on their applications in various fields. Let me check if I'm covering all key points without being too technical. Keep it concise, around 3-4 sentences. Make sure it's clear and easy to understand.\n",
            "</think>\n",
            "\n",
            "Large language models (LLMs) are AI systems designed to understand and generate human language. They are trained on vast datasets to learn complex patterns and nuances, enabling them to comprehend context, understand emotions, and generate coherent text. These models are used for tasks like language translation, content creation, and customer service, demonstrating their adaptability and efficiency in real-world applications."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. GRPO Training for LLM"
      ],
      "metadata": {
        "id": "XoINPlYPiTCL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc2081ab",
        "outputId": "e25cd0e2-23b4-4dd3-ee37-5a22c61cb7a6"
      },
      "source": [
        "!pip install reasoning_gym"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting reasoning_gym\n",
            "  Downloading reasoning_gym-0.1.23-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting arckit==0.1.0 (from reasoning_gym)\n",
            "  Downloading arckit-0.1.0-py3-none-any.whl.metadata (503 bytes)\n",
            "Collecting bfi==1.0.4 (from reasoning_gym)\n",
            "  Downloading bfi-1.0.4-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting cellpylib==2.4.0 (from reasoning_gym)\n",
            "  Downloading cellpylib-2.4.0.tar.gz (38 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting magiccube==0.3.0 (from reasoning_gym)\n",
            "  Downloading magiccube-0.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting pycosat==0.6.6 (from reasoning_gym)\n",
            "  Downloading pycosat-0.6.6.tar.gz (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyfiglet==1.0.2 (from reasoning_gym)\n",
            "  Downloading pyfiglet-1.0.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: pytz>=2024.1 in /usr/local/lib/python3.12/dist-packages (from reasoning_gym) (2025.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.2 in /usr/local/lib/python3.12/dist-packages (from reasoning_gym) (6.0.2)\n",
            "Requirement already satisfied: sympy>=1.13.1 in /usr/local/lib/python3.12/dist-packages (from reasoning_gym) (1.13.3)\n",
            "Requirement already satisfied: tabulate==0.9.0 in /usr/local/lib/python3.12/dist-packages (from reasoning_gym) (0.9.0)\n",
            "Collecting zss>=1.2.0 (from reasoning_gym)\n",
            "  Downloading zss-1.2.0.tar.gz (9.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from arckit==0.1.0->reasoning_gym) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from arckit==0.1.0->reasoning_gym) (13.9.4)\n",
            "Collecting drawsvg (from arckit==0.1.0->reasoning_gym)\n",
            "  Downloading drawsvg-2.4.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: matplotlib>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from cellpylib==2.4.0->reasoning_gym) (3.10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.1->reasoning_gym) (1.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from zss>=1.2.0->reasoning_gym) (1.17.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.2->cellpylib==2.4.0->reasoning_gym) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.2->cellpylib==2.4.0->reasoning_gym) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.2->cellpylib==2.4.0->reasoning_gym) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.2->cellpylib==2.4.0->reasoning_gym) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.2->cellpylib==2.4.0->reasoning_gym) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.2->cellpylib==2.4.0->reasoning_gym) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.2->cellpylib==2.4.0->reasoning_gym) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.2->cellpylib==2.4.0->reasoning_gym) (2.9.0.post0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->arckit==0.1.0->reasoning_gym) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->arckit==0.1.0->reasoning_gym) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->arckit==0.1.0->reasoning_gym) (0.1.2)\n",
            "Downloading reasoning_gym-0.1.23-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arckit-0.1.0-py3-none-any.whl (730 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m730.3/730.3 kB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bfi-1.0.4-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.2/159.2 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading magiccube-0.3.0-py3-none-any.whl (16 kB)\n",
            "Downloading pyfiglet-1.0.2-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading drawsvg-2.4.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: cellpylib, pycosat, zss\n",
            "  Building wheel for cellpylib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cellpylib: filename=cellpylib-2.4.0-py3-none-any.whl size=37922 sha256=fdf707b7cb62ece3966743dac857d9ddb19397ffaf302654f7407b9e2de7fecb\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/61/57/bbbbd5e8b79d6898242d075bd552bafab484034c3fcf710177\n",
            "  Building wheel for pycosat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycosat: filename=pycosat-0.6.6-cp312-cp312-linux_x86_64.whl size=170301 sha256=86132c3a35e75602393a9ce93c6b5abfd843aa6c4a0c2ffa82a7b68a383117c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/a2/34/2e/81095f4bfa4d06004e3bebc7e415733c40f0d0ab583100d3af\n",
            "  Building wheel for zss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for zss: filename=zss-1.2.0-py3-none-any.whl size=6725 sha256=5d73920b06cbb11132a04407808c8f852f3a91c832a58ae434e607e1c8db087c\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/e7/2e/44fb39352ad468427a7528cacbefefaa438a898dfd1ad2eaa4\n",
            "Successfully built cellpylib pycosat zss\n",
            "Installing collected packages: pycosat, drawsvg, bfi, zss, pyfiglet, magiccube, cellpylib, arckit, reasoning_gym\n",
            "Successfully installed arckit-0.1.0 bfi-1.0.4 cellpylib-2.4.0 drawsvg-2.4.0 magiccube-0.3.0 pycosat-0.6.6 pyfiglet-1.0.2 reasoning_gym-0.1.23 zss-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import reasoning_gym\n",
        "\n",
        "from tqdm import tqdm\n",
        "from typing import Generator, Optional, Tuple, Dict, List\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import wandb"
      ],
      "metadata": {
        "id": "ok06uOO3ib9P"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper function"
      ],
      "metadata": {
        "id": "viIXEqIrjBP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility functions\n",
        "def extract_answer_with_regex(text: str) -> str:\n",
        "    \"\"\"Extract text between <answer> and </answer> tags using regex\"\"\"\n",
        "    pattern = r'<answer>(.*?)</answer>'\n",
        "    match = re.search(pattern, text, re.DOTALL)\n",
        "    if match:\n",
        "        return match.group(1).strip()\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "def extract_thinking_with_regex(text: str) -> str:\n",
        "    \"\"\"Extract text between <think> and </think> tags using regex\"\"\"\n",
        "    pattern = r'<think>(.*?)</think>'\n",
        "    match = re.search(pattern, text, re.DOTALL)\n",
        "    if match:\n",
        "        return match.group(1).strip()\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "def sample_batch(dataset_list: List[Dict], batch_size: int) -> List[Dict]:\n",
        "    \"\"\"Sample a batch from the dataset\"\"\"\n",
        "    return random.sample(dataset_list, min(batch_size, len(dataset_list)))"
      ],
      "metadata": {
        "id": "KDsd1-G2jDM5"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GRPO Config"
      ],
      "metadata": {
        "id": "0fRhHOlqez0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class GRPOConfig:\n",
        "    \"\"\"Configuration class for GRPO (Generalized Reward-based Policy Optimization) hyperparameters\"\"\"\n",
        "\n",
        "    # Model and training hyperparameters\n",
        "    model_name: str = \"Qwen/Qwen3-0.6B\"\n",
        "    learning_rate: float = 3e-4\n",
        "    batch_size: int = 32\n",
        "    num_updates: int = 1000\n",
        "    max_steps: int = 20\n",
        "    n_outputs: int = 4\n",
        "    max_length: int = 256\n",
        "    grpo_iterations: int = 4  # Number of GRPO iterations per update\n",
        "\n",
        "    # GRPO specific hyperparameters\n",
        "    clip_epsilon: float = 0.2  # PPO clipping parameter\n",
        "    kl_beta: float = 0.02      # KL divergence coefficient\n",
        "\n",
        "    # Training configuration\n",
        "    # gradient_accumulation_steps: int = 1\n",
        "    # warmup_steps: int = 100\n",
        "    # max_grad_norm: float = 1.0\n",
        "    seed: int = 42\n",
        "\n",
        "    # Dataset configuration\n",
        "    dataset_name: str = \"syllogism\"\n",
        "    dataset_size: int = 1000\n",
        "\n",
        "    # Device configuration\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Optimization\n",
        "    adam_epsilon: float = 1e-8\n",
        "    weight_decay: float = 0.01\n",
        "\n",
        "    # Logging and saving\n",
        "    log_interval: int = 10\n",
        "    save_interval: int = 100\n",
        "    eval_interval: int = 50\n",
        "\n",
        "    # Generation parameters\n",
        "    temperature: float = 1.0\n",
        "    top_p: float = 0.9\n",
        "    top_k: int = 50\n",
        "    do_sample: bool = True"
      ],
      "metadata": {
        "id": "9hTs4m-cieu2"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize configuration\n",
        "config = GRPOConfig()\n",
        "\n",
        "# Display configuration\n",
        "print(\"GRPO Configuration:\")\n",
        "print(\"=\" * 50)\n",
        "for field, value in config.__dict__.items():\n",
        "    print(f\"{field:<25}: {value}\")\n",
        "print(\"=\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5-B4sZOilON",
        "outputId": "08c54ddc-957b-46a8-d52f-e48cde89c530"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GRPO Configuration:\n",
            "==================================================\n",
            "model_name               : Qwen/Qwen3-0.6B\n",
            "learning_rate            : 0.0003\n",
            "batch_size               : 32\n",
            "num_updates              : 1000\n",
            "max_steps                : 20\n",
            "n_outputs                : 4\n",
            "max_length               : 256\n",
            "grpo_iterations          : 4\n",
            "clip_epsilon             : 0.2\n",
            "kl_beta                  : 0.02\n",
            "seed                     : 42\n",
            "dataset_name             : syllogism\n",
            "dataset_size             : 1000\n",
            "device                   : cuda\n",
            "adam_epsilon             : 1e-08\n",
            "weight_decay             : 0.01\n",
            "log_interval             : 10\n",
            "save_interval            : 100\n",
            "eval_interval            : 50\n",
            "temperature              : 1.0\n",
            "top_p                    : 0.9\n",
            "top_k                    : 50\n",
            "do_sample                : True\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Set"
      ],
      "metadata": {
        "id": "q6qpnbwr0Ms0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset using configuration\n",
        "\n",
        "dataset = reasoning_gym.create_dataset(config.dataset_name, size=config.dataset_size, seed=config.seed)\n",
        "\n",
        "print(f\"Dataset loaded: {config.dataset_name}\")\n",
        "print(f\"Dataset size: {config.dataset_size}\")\n",
        "print(f\"Sample data point:\")\n",
        "for i, data in enumerate(dataset):\n",
        "    print(f\"Question: {data['question']}\")\n",
        "    print(f\"Answer: {data['answer']}\")\n",
        "    if i == 0:  # Show only first sample\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMuZ5V7mj5_N",
        "outputId": "00d99f60-e7ac-44d0-e4b2-4035667d3575"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded: syllogism\n",
            "Dataset size: 1000\n",
            "Sample data point:\n",
            "Question: Consider these statements:\n",
            "1. No students are humans\n",
            "2. All humans are chefs\n",
            "\n",
            "Does it logically follow that:\n",
            "Some chefs are humans?\n",
            "(Answer Yes or No)\n",
            "Answer: Yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for data in dataset:\n",
        "    print(data)\n",
        "    # Prepare the input\n",
        "    input_text = data['question'] + \" \" + data['answer']\n",
        "    inputs = tokenizer.encode(input_text)\n",
        "\n",
        "    # Generate output\n",
        "    outputs = model(torch.tensor([inputs], device=device))\n",
        "\n",
        "    # Decode the output\n",
        "    generated_text = tokenizer.decode(torch.argmax(outputs[:, -1, :], dim=-1).tolist())\n",
        "\n",
        "    print(f\"Input: {input_text}\\nGenerated: {generated_text}\\n\")\n",
        "    break"
      ],
      "metadata": {
        "id": "sDww95MQj8zM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78b6f199-dabe-49e1-b360-e13e85a75e8c"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'question': 'Consider these statements:\\n1. No students are humans\\n2. All humans are chefs\\n\\nDoes it logically follow that:\\nSome chefs are humans?\\n(Answer Yes or No)', 'answer': 'Yes', 'metadata': {'source_dataset': 'syllogism', 'source_index': 0, 'premise1': 'No students are humans', 'premise2': 'All humans are chefs', 'selected_premise': 2, 'conclusion': 'Some chefs are humans', 'is_valid': True, 'type': 'inversion'}}\n",
            "Input: Consider these statements:\n",
            "1. No students are humans\n",
            "2. All humans are chefs\n",
            "\n",
            "Does it logically follow that:\n",
            "Some chefs are humans?\n",
            "(Answer Yes or No) Yes\n",
            "Generated: <think>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset Creation and Sampling\n",
        "\n",
        "# Check what methods are available on the dataset\n",
        "print(\"Dataset type:\", type(dataset))\n",
        "print(\"Available methods:\", [method for method in dir(dataset) if not method.startswith('_')])\n",
        "\n",
        "# Since procedural datasets don't have .sample(), we need to use random sampling\n",
        "import random\n",
        "\n",
        "def sample_from_dataset(dataset, n):\n",
        "    \"\"\"Sample n items from the dataset\"\"\"\n",
        "    # Convert dataset to list if it's iterable\n",
        "    dataset_list = list(dataset) if hasattr(dataset, '__iter__') else dataset\n",
        "\n",
        "    # If the dataset is smaller than n, return all items\n",
        "    if len(dataset_list) < n:\n",
        "        return dataset_list\n",
        "\n",
        "    # Random sample without replacement\n",
        "    return random.sample(dataset_list, n)\n",
        "\n",
        "# Sample 5 data points for demonstration\n",
        "batch = sample_from_dataset(dataset, 5)\n",
        "print(f\"Sampled batch of {len(batch)} items:\")\n",
        "for i, item in enumerate(batch):\n",
        "    print(f\"Item {i+1}: {item}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uA6CVr5QxoFY",
        "outputId": "6ae9f95f-5985-4dfc-d047-b2512f4d4210"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset type: <class 'reasoning_gym.logic.syllogisms.SyllogismDataset'>\n",
            "Available methods: ['DEFAULT_TERMS', 'category', 'config', 'score_answer', 'seed', 'size', 'terms']\n",
            "Sampled batch of 5 items:\n",
            "Item 1: {'question': 'Consider these statements:\\n1. Some doctors are not insects\\n2. No insects are humans\\n\\nDoes it logically follow that:\\nNo humans are insects?\\n(Answer Yes or No)', 'answer': 'Yes', 'metadata': {'source_dataset': 'syllogism', 'source_index': 417, 'premise1': 'Some doctors are not insects', 'premise2': 'No insects are humans', 'selected_premise': 2, 'conclusion': 'No humans are insects', 'is_valid': True, 'type': 'inversion'}}\n",
            "Item 2: {'question': 'Consider these statements:\\n1. Some tigers are not writers\\n2. All writers are dolphins\\n\\nDoes it logically follow that:\\nSome dolphins are writers?\\n(Answer Yes or No)', 'answer': 'Yes', 'metadata': {'source_dataset': 'syllogism', 'source_index': 119, 'premise1': 'Some tigers are not writers', 'premise2': 'All writers are dolphins', 'selected_premise': 2, 'conclusion': 'Some dolphins are writers', 'is_valid': True, 'type': 'inversion'}}\n",
            "Item 3: {'question': 'Consider these statements:\\n1. Some insects are birds\\n2. No birds are grandparents\\n\\nDoes it logically follow that:\\nSome insects are not grandparents?\\n(Answer Yes or No)', 'answer': 'Yes', 'metadata': {'source_dataset': 'syllogism', 'source_index': 535, 'premise1': 'Some insects are birds', 'premise2': 'No birds are grandparents', 'conclusion': 'Some insects are not grandparents', 'is_valid': True, 'type': 'syllogism'}}\n",
            "Item 4: {'question': 'Consider these statements:\\n1. No writers are adults\\n2. No adults are children\\n\\nDoes it logically follow that:\\nNo adults are writers?\\n(Answer Yes or No)', 'answer': 'Yes', 'metadata': {'source_dataset': 'syllogism', 'source_index': 983, 'premise1': 'No writers are adults', 'premise2': 'No adults are children', 'selected_premise': 1, 'conclusion': 'No adults are writers', 'is_valid': True, 'type': 'inversion'}}\n",
            "Item 5: {'question': 'Consider these statements:\\n1. Some teachers are reptiles\\n2. Some reptiles are not spiders\\n\\nDoes it logically follow that:\\nSome teachers are not spiders?\\n(Answer Yes or No)', 'answer': 'No', 'metadata': {'source_dataset': 'syllogism', 'source_index': 629, 'premise1': 'Some teachers are reptiles', 'premise2': 'Some reptiles are not spiders', 'conclusion': 'Some teachers are not spiders', 'is_valid': False, 'type': 'syllogism'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GRPO"
      ],
      "metadata": {
        "id": "9q9o_XhUgF9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRPO:\n",
        "    def __init__(self, model, tokenizer, config: GRPOConfig):\n",
        "        super(GRPO, self).__init__()\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.config = config\n",
        "\n",
        "        # Move model to device\n",
        "        self.model.to(config.device)\n",
        "\n",
        "    def loss_function(self, old_log_probs: torch.Tensor, new_log_probs: torch.Tensor, advantage: float) -> torch.Tensor:\n",
        "        \"\"\"Compute GRPO loss with clipping\"\"\"\n",
        "        if len(old_log_probs) == 0 or len(new_log_probs) == 0:\n",
        "            return torch.tensor(0.0, device=self.config.device, requires_grad=True)\n",
        "\n",
        "        # Move tensors to same device\n",
        "        old_log_probs = old_log_probs.to(self.config.device)\n",
        "        new_log_probs = new_log_probs.to(self.config.device)\n",
        "\n",
        "        # Sum log probabilities for the sequence\n",
        "        old_log_prob_sum = old_log_probs.sum()\n",
        "        new_log_prob_sum = new_log_probs.sum()\n",
        "\n",
        "        # Compute ratio\n",
        "        ratio = torch.exp(new_log_prob_sum - old_log_prob_sum)\n",
        "\n",
        "        # Compute clipped surrogate loss\n",
        "        advantage_tensor = torch.tensor(advantage, device=self.config.device)\n",
        "        surr1 = ratio * advantage_tensor\n",
        "        surr2 = torch.clamp(ratio, 1 - self.config.clip_epsilon, 1 + self.config.clip_epsilon) * advantage_tensor\n",
        "\n",
        "        # Return negative because we want to maximize\n",
        "        loss = -torch.min(surr1, surr2)\n",
        "        return loss\n",
        "\n",
        "    def compute_advantages(self, rewards: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Compute advantages with proper normalization\"\"\"\n",
        "        # Simple advantage computation - can be made more sophisticated\n",
        "        normalized_rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-8)\n",
        "        return normalized_rewards\n",
        "\n",
        "    def generate_with_log_probs(self, input_text: str, max_new_tokens: int = 100) -> Tuple[str, torch.Tensor, List[int], List[int]]:\n",
        "        \"\"\"Generate text while tracking log probabilities for each token\"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            input_token_ids = self.tokenizer.encode(input_text)\n",
        "            input_tensor = torch.tensor([input_token_ids], device=self.config.device)\n",
        "\n",
        "            cache = KVCache(n_layers=self.model.cfg[\"n_layers\"])\n",
        "            self.model.reset_kv_cache()\n",
        "\n",
        "            # Initialize with input tokens\n",
        "            generated_tokens = []\n",
        "            log_probs = []\n",
        "\n",
        "            # Prime the cache\n",
        "            logits = self.model(input_tensor, cache=cache)\n",
        "\n",
        "            for _ in range(max_new_tokens):\n",
        "                # Get probabilities for the last token\n",
        "                if self.config.temperature != 1.0:\n",
        "                    logits_scaled = logits[:, -1, :] / self.config.temperature\n",
        "                else:\n",
        "                    logits_scaled = logits[:, -1, :]\n",
        "\n",
        "                probs = F.softmax(logits_scaled, dim=-1)\n",
        "                log_prob_dist = F.log_softmax(logits_scaled, dim=-1)\n",
        "\n",
        "                # Sample next token\n",
        "                if self.config.do_sample:\n",
        "                    next_token = torch.multinomial(probs, 1)\n",
        "                else:\n",
        "                    next_token = torch.argmax(probs, dim=-1, keepdim=True)\n",
        "\n",
        "                # Store the log probability of the chosen token\n",
        "                token_log_prob = log_prob_dist.gather(1, next_token)\n",
        "                log_probs.append(token_log_prob.item())\n",
        "                generated_tokens.append(next_token.item())\n",
        "\n",
        "                # Check for EOS\n",
        "                if next_token.item() == self.tokenizer.eos_token_id:\n",
        "                    break\n",
        "\n",
        "                # Continue generation\n",
        "                logits = self.model(next_token, cache=cache)\n",
        "\n",
        "            # Decode the generated text\n",
        "            full_tokens = input_token_ids + generated_tokens\n",
        "            generated_text = self.tokenizer.decode(full_tokens)\n",
        "\n",
        "            return generated_text, torch.tensor(log_probs), generated_tokens, input_token_ids\n",
        "\n",
        "    def recompute_log_probs(self, input_tokens: List[int], generated_tokens: List[int]) -> torch.Tensor:\n",
        "        \"\"\"Recompute log probabilities for a generated sequence\"\"\"\n",
        "        if not generated_tokens:\n",
        "            return torch.tensor([])\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Prepare full sequence\n",
        "            full_sequence = input_tokens + generated_tokens\n",
        "            input_tensor = torch.tensor([full_sequence], device=self.config.device)\n",
        "\n",
        "            # Get logits for the sequence\n",
        "            logits = self.model(input_tensor)\n",
        "\n",
        "            # Extract logits for generated tokens (offset by input length)\n",
        "            start_idx = len(input_tokens) - 1\n",
        "            end_idx = start_idx + len(generated_tokens)\n",
        "\n",
        "            if end_idx > logits.shape[1]:\n",
        "                end_idx = logits.shape[1]\n",
        "\n",
        "            gen_logits = logits[0, start_idx:end_idx, :]\n",
        "\n",
        "            # Apply temperature if configured\n",
        "            if self.config.temperature != 1.0:\n",
        "                gen_logits = gen_logits / self.config.temperature\n",
        "\n",
        "            # Compute log probabilities\n",
        "            log_probs = F.log_softmax(gen_logits, dim=-1)\n",
        "\n",
        "            # Get log probs for actual generated tokens\n",
        "            token_log_probs = []\n",
        "            for i, token_id in enumerate(generated_tokens):\n",
        "                if i < log_probs.shape[0]:\n",
        "                    token_log_probs.append(log_probs[i, token_id].item())\n",
        "\n",
        "            return torch.tensor(token_log_probs, device=self.config.device)"
      ],
      "metadata": {
        "id": "Aboz7t_9xiX0"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trainner"
      ],
      "metadata": {
        "id": "8WS-CDUMgNP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRPOTrainer:\n",
        "    def __init__(self, model, tokenizer, config, dataset):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.config = config\n",
        "        self.dataset = dataset\n",
        "        self.dataset_list = list(dataset)\n",
        "\n",
        "        # Initialize GRPO\n",
        "        self.grpo = GRPO(model, tokenizer, config)\n",
        "\n",
        "        # Initialize optimizer\n",
        "        self.optimizer = optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=config.learning_rate,\n",
        "            weight_decay=config.weight_decay,\n",
        "            eps=config.adam_epsilon\n",
        "        )\n",
        "\n",
        "        print(f\"Initialized GRPO trainer with dataset size: {len(self.dataset_list)}\")\n",
        "\n",
        "    def train_step(self, batch: List[Dict]) -> Dict:\n",
        "        \"\"\"Single training step\"\"\"\n",
        "        # Generate responses and collect trajectories\n",
        "        trajectories = []\n",
        "        all_rewards = []\n",
        "\n",
        "        for data in batch:\n",
        "            question = data['question']\n",
        "            true_answer = data['answer']\n",
        "\n",
        "            # Generate multiple responses per question\n",
        "            for _ in range(self.config.n_outputs):\n",
        "                # Generate response with log probs\n",
        "                generated_text, old_log_probs, generated_tokens, input_tokens = self.grpo.generate_with_log_probs(\n",
        "                    question, max_new_tokens=self.config.max_length\n",
        "                )\n",
        "\n",
        "                # Extract answer\n",
        "                extracted_answer = extract_answer_with_regex(generated_text)\n",
        "                thinking_content = extract_thinking_with_regex(generated_text)\n",
        "\n",
        "                # Compute reward components\n",
        "                try:\n",
        "                    accuracy = 1.0 if self.dataset.score_answer(extracted_answer, true_answer) == 1.0 else 0.0\n",
        "                except:\n",
        "                    # Fallback: simple string matching\n",
        "                    accuracy = 1.0 if extracted_answer.strip().lower() == true_answer.strip().lower() else 0.0\n",
        "\n",
        "                format_reward = 0.3 if extracted_answer.strip() else 0.0\n",
        "                thinking_reward = 0.2 if thinking_content.strip() else 0.0\n",
        "                total_reward = accuracy + format_reward + thinking_reward\n",
        "\n",
        "                trajectories.append({\n",
        "                    'question': question,\n",
        "                    'true_answer': true_answer,\n",
        "                    'generated_text': generated_text,\n",
        "                    'extracted_answer': extracted_answer,\n",
        "                    'old_log_probs': old_log_probs,\n",
        "                    'generated_tokens': generated_tokens,\n",
        "                    'input_tokens': input_tokens,\n",
        "                    'reward': total_reward,\n",
        "                    'accuracy': accuracy\n",
        "                })\n",
        "                all_rewards.append(total_reward)\n",
        "\n",
        "        if not trajectories:\n",
        "            return {'avg_reward': 0, 'avg_accuracy': 0, 'avg_loss': 0}\n",
        "\n",
        "        # Compute advantages\n",
        "        rewards_tensor = torch.tensor(all_rewards, dtype=torch.float32, device=self.config.device)\n",
        "        advantages = self.grpo.compute_advantages(rewards_tensor)\n",
        "\n",
        "        # GRPO update iterations\n",
        "        total_loss = 0\n",
        "        valid_losses = 0\n",
        "\n",
        "        for grpo_iter in range(self.config.grpo_iterations):\n",
        "            self.model.train()\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            batch_loss = torch.tensor(0.0, device=self.config.device, requires_grad=True)\n",
        "\n",
        "            for i, traj in enumerate(trajectories):\n",
        "                # Recompute log probs with current model\n",
        "                new_log_probs = self.grpo.recompute_log_probs(\n",
        "                    traj['input_tokens'],\n",
        "                    traj['generated_tokens']\n",
        "                )\n",
        "\n",
        "                # Skip if lengths don't match or empty\n",
        "                if len(new_log_probs) != len(traj['old_log_probs']) or len(new_log_probs) == 0:\n",
        "                    continue\n",
        "\n",
        "                # Compute loss\n",
        "                loss = self.grpo.loss_function(\n",
        "                    traj['old_log_probs'],\n",
        "                    new_log_probs,\n",
        "                    advantages[i].item()\n",
        "                )\n",
        "\n",
        "                batch_loss = batch_loss + loss\n",
        "                valid_losses += 1\n",
        "\n",
        "            if valid_losses > 0:\n",
        "                # Average loss and backward pass\n",
        "                avg_loss = batch_loss / valid_losses\n",
        "                total_loss += avg_loss.item()\n",
        "\n",
        "                avg_loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "                self.optimizer.step()\n",
        "\n",
        "        # Compute metrics\n",
        "        avg_reward = np.mean(all_rewards)\n",
        "        avg_accuracy = np.mean([t['accuracy'] for t in trajectories])\n",
        "        avg_loss = total_loss / self.config.grpo_iterations if self.config.grpo_iterations > 0 else 0\n",
        "\n",
        "        return {\n",
        "            'avg_reward': avg_reward,\n",
        "            'avg_accuracy': avg_accuracy,\n",
        "            'avg_loss': avg_loss,\n",
        "            'trajectories': trajectories\n",
        "        }\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Main training loop\"\"\"\n",
        "        print(\"Starting GRPO training...\")\n",
        "\n",
        "        for update in tqdm(range(self.config.num_updates), desc=\"GRPO Training\"):\n",
        "            # Sample batch\n",
        "            batch = sample_batch(self.dataset_list, self.config.batch_size)\n",
        "\n",
        "            # Training step\n",
        "            step_results = self.train_step(batch)\n",
        "\n",
        "            # Logging\n",
        "            if update % self.config.log_interval == 0:\n",
        "                print(f\"\\nUpdate {update:4d}\")\n",
        "                print(f\"  Avg Reward: {step_results['avg_reward']:.3f}\")\n",
        "                print(f\"  Avg Accuracy: {step_results['avg_accuracy']:.3f}\")\n",
        "                print(f\"  Avg Loss: {step_results['avg_loss']:.4f}\")\n",
        "\n",
        "                # Show example\n",
        "                if 'trajectories' in step_results and step_results['trajectories']:\n",
        "                    example = step_results['trajectories'][0]\n",
        "                    print(f\"  Example Q: {example['question'][:60]}...\")\n",
        "                    print(f\"  Example Generated A: {example['extracted_answer'][:40]}...\")\n",
        "                    print(f\"  Example True A: {example['true_answer'][:40]}...\")\n",
        "                    print(\"-\" * 60)\n",
        "\n",
        "        print(\"GRPO training completed!\")\n",
        "        return self.model"
      ],
      "metadata": {
        "id": "TdEjCLuJdoce"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and run training\n",
        "print(\"Starting GRPO training with fixed implementation...\")\n",
        "\n",
        "# Create trainer instance\n",
        "trainer = GRPOTrainer(model, tokenizer, config, dataset)\n",
        "\n",
        "# Run training\n",
        "print(\"=\"*60)\n",
        "print(\"GRPO Training Starting\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "trained_model = trainer.train()\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"GRPO Training Complete!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667
        },
        "id": "XH1g4-ute6gm",
        "outputId": "b0da6a7c-244d-40bc-8ed6-099bf11afad4"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting GRPO training with fixed implementation...\n",
            "Initialized GRPO trainer with dataset size: 1000\n",
            "============================================================\n",
            "GRPO Training Starting\n",
            "============================================================\n",
            "Starting GRPO training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "GRPO Training:   0%|          | 1/1000 [22:18<371:22:06, 1338.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Update    0\n",
            "  Avg Reward: 0.002\n",
            "  Avg Accuracy: 0.000\n",
            "  Avg Loss: 0.0458\n",
            "  Example Q: Consider these statements:\n",
            "1. Some bees are insects\n",
            "2. No in...\n",
            "  Example Generated A: ...\n",
            "  Example True A: Yes...\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "GRPO Training:   0%|          | 3/1000 [1:10:09<388:36:51, 1403.22s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2604517643.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1037412695.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;31m# Training step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0mstep_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;31m# Logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1037412695.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;31m# Generate response with log probs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 generated_text, old_log_probs, generated_tokens, input_tokens = self.grpo.generate_with_log_probs(\n\u001b[0m\u001b[1;32m     36\u001b[0m                     \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 )\n",
            "\u001b[0;32m/tmp/ipython-input-675204939.py\u001b[0m in \u001b[0;36mgenerate_with_log_probs\u001b[0;34m(self, input_text, max_new_tokens)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;31m# Continue generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;31m# Decode the generated text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3033320186.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, in_idx, cache)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrf_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mblk_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             x, new_blk_cache = block(x, mask, self.cos, self.sin,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                      \u001b[0mstart_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                                      cache=blk_cache)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1067262297.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask, cos, sin, start_pos, cache)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mshortcut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Shape [batch_size, num_tokens, emb_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mshortcut\u001b[0m  \u001b[0;31m# Add the original input back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4012022366.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask, cos, sin, start_pos, cache)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# Apply RoPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mqueries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_rope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mkeys_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_rope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-153761810.py\u001b[0m in \u001b[0;36mapply_rope\u001b[0;34m(x, cos, sin, offset)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# It's ok to use lower-precision after applying cos and sin rotation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mx_rotated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Test the trained model\n",
        "print(\"\\nTesting trained model with a sample question:\")\n",
        "test_question = \"All roses are flowers. Some flowers are red. Therefore, some roses are red.\"\n",
        "test_input = tokenizer.encode(test_question)\n",
        "test_tensor = torch.tensor([test_input], device=device)\n",
        "\n",
        "print(f\"Test question: {test_question}\")\n",
        "print(\"Generated response:\")\n",
        "\n",
        "# Generate response\n",
        "for token in generate_text_basic_stream(\n",
        "    model=trained_model,\n",
        "    token_ids=test_tensor,\n",
        "    max_new_tokens=200,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        "):\n",
        "    token_id = token.squeeze(0).tolist()\n",
        "    print(tokenizer.decode(token_id), end=\"\", flush=True)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)"
      ],
      "metadata": {
        "id": "ZZ6cxxyUpbZP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}